{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ãƒ‡ãƒ¼ã‚¿æ¦‚è¦\n",
    "å›½å‹¢èª¿æŸ»ã¯ã€æ—¥æœ¬ã«ä½ã‚“ã§ã„ã‚‹ã™ã¹ã¦ã®äººã¨ä¸–å¸¯ã‚’å¯¾è±¡ã¨ã™ã‚‹å›½ã®æœ€ã‚‚é‡è¦ãªçµ±è¨ˆèª¿æŸ»ã§ã€ï¼•å¹´ã”ã¨ã«å®Ÿæ–½ã•ã‚Œã¾ã™ã€‚å›½å‹¢èª¿æŸ»ã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹æ—¥æœ¬ã®äººå£ã‚„ä¸–å¸¯ã®å®Ÿæ…‹ã¯ã€å›½ã‚„åœ°æ–¹å…¬å…±å›£ä½“ã®è¡Œæ”¿ã«ãŠã„ã¦åˆ©ç”¨ã•ã‚Œã‚‹ã“ã¨ã¯ã‚‚ã¨ã‚ˆã‚Šã€æ°‘é–“ä¼æ¥­ã‚„ç ”ç©¶æ©Ÿé–¢ã§ã‚‚åºƒãåˆ©ç”¨ã•ã‚Œã€ãã®ã‚ˆã†ãªåˆ©ç”¨ã‚’é€šã˜ã¦å›½æ°‘ç”Ÿæ´»ã«å½¹ç«‹ã¦ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "ã€€å›½å‹¢èª¿æŸ»ã§ã¯ã€å¹´é½¢åˆ¥ã®äººå£ã€å®¶æ—æ§‹æˆã€åƒã„ã¦ã„ã‚‹äººã‚„æ—¥æœ¬ã«ä½ã‚“ã§ã„ã‚‹å¤–å›½äººãªã©ã®çµæœã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚\n",
    "//\n",
    "https://www.e-stat.go.jp/stat-search/database?page=1&layout=datalist&toukei=00200521&tstat=000001011777&cycle=0&tclass1=000001011778&statdisp_id=0003410379&tclass2val=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸‹è¨˜ã®æ¦‚è¦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e-Stat APIã‹ã‚‰äººå£ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€å¿…è¦ãªæƒ…å ±ã‚’æŠ½å‡ºãƒ»æ•´å½¢ã—ã¦ã€BigQueryã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å‡¦ç†ã‚’è¡Œã„ã¾ã™ã€‚Google Cloudã®èªè¨¼è¨­å®šã¨ã€APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å‡¦ç†ã€BigQueryã¸ã®ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...\n",
      "ğŸ§¹ ãƒ‡ãƒ¼ã‚¿æ•´å½¢ä¸­...\n",
      "â˜ï¸ BigQuery ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...\n",
      "ğŸ†• ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ 20250413_estat_dataset ã‚’ä½œæˆã—ã¾ã—ãŸã€‚\n",
      "âœ… BigQuery ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: just-episode-454309-a0.20250413_estat_dataset.20250413_estat_population\n",
      "ğŸ‰ å‡¦ç†å®Œäº†ï¼\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import GoogleCloudError\n",
    "from typing import Dict, List\n",
    "\n",
    "# === å®šæ•° ===\n",
    "PROJECT_ID = \"just-episode-454309-a0\"\n",
    "DATASET_ID = \"20250413_estat_dataset\"\n",
    "TABLE_ID = \"20250413_estat_population\"\n",
    "APP_ID = \"ã‚³ãƒ¼ãƒ‰å–å¾—å¿…è¦\"  # è‡ªåˆ†ã® App ID ã«å¤‰æ›´\n",
    "STATS_DATA_ID = \"0003410379\"\n",
    "TARGET_AREAS = [\"13000\", \"14000\", \"26000\", \"27000\", \"40000\", \"41000\"]\n",
    "AREA_DICT = {\n",
    "    '40000': 'ç¦å²¡çœŒ',\n",
    "    '13000': 'æ±äº¬éƒ½',\n",
    "    '14000': 'ç¥å¥ˆå·çœŒ',\n",
    "    '26000': 'äº¬éƒ½åºœ',\n",
    "    '27000': 'å¤§é˜ªåºœ',\n",
    "    '41000': 'ä½è³€çœŒ'\n",
    "}\n",
    "\n",
    "# === BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ ===\n",
    "client = bigquery.Client()\n",
    "\n",
    "# === èªè¨¼æƒ…å ±ã®è¨­å®š ===\n",
    "# èªè¨¼æƒ…å ±è¨­å®š\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/kobayashitoshiyuki/Desktop/estat/ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—å¿…è¦.json\"\n",
    "\n",
    "def fetch_estat_data(app_id: str, stats_data_id: str) -> dict:\n",
    "    \"\"\"e-Stat API ã‹ã‚‰çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—\"\"\"\n",
    "    url = \"http://api.e-stat.go.jp/rest/3.0/app/json/getStatsData\"\n",
    "    params = {\n",
    "        \"appId\": app_id,\n",
    "        \"lang\": \"J\",\n",
    "        \"statsDataId\": stats_data_id,\n",
    "        \"metaGetFlg\": \"Y\",\n",
    "        \"cntGetFlg\": \"N\",\n",
    "        \"explanationGetFlg\": \"Y\",\n",
    "        \"annotationGetFlg\": \"Y\",\n",
    "        \"sectionHeaderFlg\": \"1\",\n",
    "        \"replaceSpChars\": \"0\"\n",
    "    }\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        raise SystemExit(f\"APIå–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n",
    "\n",
    "def extract_population_data(data: dict) -> pd.DataFrame:\n",
    "    \"\"\"APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰äººå£ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºãƒ»æ•´å½¢\"\"\"\n",
    "    # æ€§åˆ¥ãƒã‚¹ã‚¿\n",
    "    sex_class = pd.json_normalize(data['GET_STATS_DATA']['STATISTICAL_DATA']['CLASS_INF']['CLASS_OBJ'][1])\n",
    "    sex_expanded = pd.DataFrame(sex_class[\"CLASS\"][0])\n",
    "    sex_expanded[\"category_id\"] = sex_class[\"@id\"][0]\n",
    "    sex_expanded[\"category_name\"] = sex_class[\"@name\"][0]\n",
    "    sex_expanded.columns = sex_expanded.columns.str.replace('@', '', 1)\n",
    "\n",
    "    # äººå£ãƒ‡ãƒ¼ã‚¿\n",
    "    values = pd.json_normalize(data['GET_STATS_DATA']['STATISTICAL_DATA']['DATA_INF']['VALUE'])\n",
    "    values.columns = values.columns.str.replace('@', '', 1)\n",
    "    values.rename(columns={'$': 'äººæ•°'}, inplace=True)\n",
    "\n",
    "    # éƒ½é“åºœçœŒãƒ»æ€§åˆ¥ãªã©æ•´å½¢\n",
    "    dat = values.query(\"area in @TARGET_AREAS\").copy()\n",
    "    dat[\"çœŒå\"] = dat[\"area\"].map(AREA_DICT)\n",
    "    dat[\"å¹´\"] = dat[\"time\"].astype(str).str[:4]\n",
    "    dat = dat.merge(sex_expanded, left_on=\"cat01\", right_on=\"code\", how=\"left\")\n",
    "    dat.rename(columns={'name': 'æ€§åˆ¥'}, inplace=True)\n",
    "    dat[\"äººæ•°\"] = pd.to_numeric(dat[\"äººæ•°\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "    dat = dat[[\"å¹´\", \"çœŒå\", \"æ€§åˆ¥\", \"äººæ•°\"]]\n",
    "\n",
    "    return dat\n",
    "\n",
    "def create_dataset_if_not_exists(project_id: str, dataset_id: str) -> None:\n",
    "    \"\"\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå­˜åœ¨ã—ãªã„å ´åˆã«ä½œæˆ\"\"\"\n",
    "    dataset_ref = client.dataset(dataset_id)\n",
    "    try:\n",
    "        client.get_dataset(dataset_ref)  # Check if dataset exists\n",
    "    except GoogleCloudError:\n",
    "        # Create dataset if it doesn't exist\n",
    "        dataset = bigquery.Dataset(dataset_ref)\n",
    "        dataset.location = \"asia-northeast1\"  # Set location\n",
    "        client.create_dataset(dataset)\n",
    "        print(f\"ğŸ†• ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ {dataset_id} ã‚’ä½œæˆã—ã¾ã—ãŸã€‚\")\n",
    "\n",
    "def load_to_bigquery(df: pd.DataFrame, project_id: str, dataset_id: str, table_id: str) -> None:\n",
    "    \"\"\"DataFrame ã‚’ BigQuery ã«ãƒ­ãƒ¼ãƒ‰\"\"\"\n",
    "    table_ref = f\"{project_id}.{dataset_id}.{table_id}\"\n",
    "    schema = [\n",
    "        bigquery.SchemaField(\"å¹´\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"çœŒå\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"æ€§åˆ¥\", \"STRING\"),\n",
    "        bigquery.SchemaField(\"äººæ•°\", \"INTEGER\"),\n",
    "    ]\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ\n",
    "    create_dataset_if_not_exists(project_id, dataset_id)\n",
    "\n",
    "    try:\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            schema=schema,\n",
    "            write_disposition=\"WRITE_TRUNCATE\",  # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä¸Šæ›¸ãã™ã‚‹è¨­å®š\n",
    "            source_format=bigquery.SourceFormat.CSV,  # CSVå½¢å¼ã§ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "            skip_leading_rows=1,  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—\n",
    "            autodetect=False,  # ã‚¹ã‚­ãƒ¼ãƒã‚’æ‰‹å‹•ã§æŒ‡å®š\n",
    "        )\n",
    "        \n",
    "        # CSV å½¢å¼ã§ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "        job = client.load_table_from_dataframe(df, table_ref, job_config=job_config)\n",
    "        job.result()  # ã‚¸ãƒ§ãƒ–ã®å®Œäº†ã‚’å¾…æ©Ÿ\n",
    "        print(f\"âœ… BigQuery ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: {table_ref}\")\n",
    "    except GoogleCloudError as e:\n",
    "        raise SystemExit(f\"BigQuery ã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}\")\n",
    "\n",
    "def main():\n",
    "    print(\"ğŸ”„ ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...\")\n",
    "    data = fetch_estat_data(APP_ID, STATS_DATA_ID)\n",
    "    \n",
    "    print(\"ğŸ§¹ ãƒ‡ãƒ¼ã‚¿æ•´å½¢ä¸­...\")\n",
    "    final_df = extract_population_data(data)\n",
    "    \n",
    "    print(\"â˜ï¸ BigQuery ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
    "    load_to_bigquery(final_df, PROJECT_ID, DATASET_ID, TABLE_ID)\n",
    "\n",
    "    print(\"ğŸ‰ å‡¦ç†å®Œäº†ï¼\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸‹è¨˜ã‚³ãƒ¼ãƒ‰ã®æ¦‚è¦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BigQuery ã‹ã‚‰äººå£ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€å¹´ãƒ»éƒ½é“åºœçœŒåˆ¥ãŠã‚ˆã³å¹´ãƒ»æ€§åˆ¥åˆ¥ã®å¹³å‡äººæ•°ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã€ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã—ã€GitHub ã«è‡ªå‹•ã§ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’è‡ªå‹•åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ BigQuery ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kobayashitoshiyuki/opt/anaconda3/envs/venv/lib/python3.8/site-packages/google/cloud/bigquery/table.py:2518: UserWarning: Unable to represent RANGE schema as struct using pandas ArrowDtype. Using `object` instead. To use ArrowDtype, use pandas >= 1.5 and pyarrow >= 10.0.1.\n",
      "  warnings.warn(_RANGE_PYARROW_WARNING)\n",
      "/Users/kobayashitoshiyuki/opt/anaconda3/envs/venv/lib/python3.8/site-packages/google/cloud/bigquery/table.py:2532: UserWarning: Unable to represent RANGE schema as struct using pandas ArrowDtype. Using `object` instead. To use ArrowDtype, use pandas >= 1.5 and pyarrow >= 10.0.1.\n",
      "  warnings.warn(_RANGE_PYARROW_WARNING)\n",
      "/Users/kobayashitoshiyuki/opt/anaconda3/envs/venv/lib/python3.8/site-packages/google/cloud/bigquery/table.py:2546: UserWarning: Unable to represent RANGE schema as struct using pandas ArrowDtype. Using `object` instead. To use ArrowDtype, use pandas >= 1.5 and pyarrow >= 10.0.1.\n",
      "  warnings.warn(_RANGE_PYARROW_WARNING)\n",
      "/Users/kobayashitoshiyuki/opt/anaconda3/envs/venv/lib/python3.8/site-packages/google/cloud/bigquery/table.py:1900: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ã‚°ãƒ©ãƒ•ä½œæˆä¸­...\n",
      "ğŸ”§ Git ãƒªãƒã‚¸ãƒˆãƒªåˆæœŸåŒ–ä¸­...\n",
      "ğŸ”§ Git ãƒªãƒã‚¸ãƒˆãƒªã‚’åˆæœŸåŒ–ä¸­...\n",
      "âœ… æ—¢ã« Git ãƒªãƒã‚¸ãƒˆãƒªã¨ã—ã¦åˆæœŸåŒ–æ¸ˆã¿\n",
      "ğŸš€ GitHub ã«ãƒ—ãƒƒã‚·ãƒ¥ä¸­...\n",
      "On branch main\n",
      "nothing to commit, working tree clean\n",
      "branch 'main' set up to track 'origin/main'.\n",
      "ğŸ‰ å®Œäº†ã—ã¾ã—ãŸï¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To github.com:g238025t/20250413.git\n",
      " * [new branch]      main -> main\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "from google.cloud import bigquery\n",
    "import db_dtypes\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/Users/kobayashitoshiyuki/Desktop/estat/ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—å¿…è¦.json\"\n",
    "\n",
    "# BigQuery ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ\n",
    "client = bigquery.Client()\n",
    "\n",
    "\n",
    "# === å®šæ•° ===\n",
    "PROJECT_ID = \"just-episode-454309-a0\"\n",
    "DATASET_ID = \"20250413_estat_dataset\"\n",
    "TABLE_ID = \"20250413_estat_population\"\n",
    "LOCAL_FOLDER = \"/Users/kobayashitoshiyuki/Desktop/estat/20250413\"\n",
    "GITHUB_REPO_PATH = \"/Users/kobayashitoshiyuki/Desktop/estat/20250413\"\n",
    "GITHUB_REMOTE_URL = \"https://github.com/g238025t/20250413.git\"  # è‡ªåˆ†ã®URLã«å¤‰æ›´ï¼\n",
    "# === BigQueryã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ ===\n",
    "client = bigquery.Client()\n",
    "\n",
    "# === BigQueryã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾— ===\n",
    "def load_data_from_bigquery() -> pd.DataFrame:\n",
    "    query = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`\n",
    "    \"\"\"\n",
    "    return client.query(query).to_dataframe()\n",
    "\n",
    "# === ã‚°ãƒ©ãƒ•ä½œæˆãƒ»ä¿å­˜ ===\n",
    "def create_and_save_graphs(df: pd.DataFrame, folder_path: str):\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    # ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆæ—¥æœ¬èªå¯¾å¿œï¼‰\n",
    "    plt.rcParams[\"font.family\"] = \"IPAexGothic\"\n",
    "\n",
    "    # çœŒã”ã¨ã®è‰²å‰²ã‚Šå½“ã¦\n",
    "    area_list = df[\"çœŒå\"].astype(str).unique()\n",
    "    colors = list(mcolors.TABLEAU_COLORS.values())[:len(area_list)]\n",
    "    area_colors = {area: color for area, color in zip(area_list, colors)}\n",
    "\n",
    "    # æ€§åˆ¥ã”ã¨ã®è‰²\n",
    "    sex_list = df[\"æ€§åˆ¥\"].astype(str).unique()\n",
    "    sex_colors = {sex: color for sex, color in zip(sex_list, [\"blue\", \"red\", \"green\"])}\n",
    "\n",
    "    # ã‚°ãƒ«ãƒ¼ãƒ—åŒ–\n",
    "    area_year_avg = df.groupby([\"å¹´\", \"çœŒå\"])[\"äººæ•°\"].mean().sort_index(level=\"å¹´\", ascending=True)\n",
    "    sex_year_avg = df.groupby([\"å¹´\", \"æ€§åˆ¥\"])[\"äººæ•°\"].mean().sort_index(level=\"å¹´\", ascending=True)\n",
    "\n",
    "    # ã‚°ãƒ©ãƒ•â‘ ï¼ˆéƒ½é“åºœçœŒåˆ¥ï¼‰\n",
    "    fig1_path = os.path.join(folder_path, \"area_year_avg.png\")\n",
    "    plt.figure(figsize=(12, 20))\n",
    "    plt.barh(\n",
    "        [f\"{year} - {area}\" for year, area in area_year_avg.index],\n",
    "        area_year_avg.values,\n",
    "        color=[area_colors[area] for _, area in area_year_avg.index]\n",
    "    )\n",
    "    handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10, label=area)\n",
    "               for area, color in area_colors.items()]\n",
    "    plt.legend(handles=handles, title=\"éƒ½é“åºœçœŒ\")\n",
    "    plt.xlabel(\"äººæ•°\")\n",
    "    plt.ylabel(\"å¹´ - çœŒå\")\n",
    "    plt.title(\"å¹³å‡äººæ•°ï¼ˆå¹´ãƒ»éƒ½é“åºœçœŒåˆ¥ï¼‰\")\n",
    "    plt.grid(axis='x', linestyle=\"--\", alpha=0.7)\n",
    "    plt.savefig(fig1_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    # ã‚°ãƒ©ãƒ•â‘¡ï¼ˆæ€§åˆ¥åˆ¥ï¼‰\n",
    "    fig2_path = os.path.join(folder_path, \"sex_year_avg.png\")\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.barh(\n",
    "        [f\"{year} - {sex}\" for year, sex in sex_year_avg.index],\n",
    "        sex_year_avg.values,\n",
    "        color=[sex_colors[sex] for _, sex in sex_year_avg.index]\n",
    "    )\n",
    "    handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10, label=sex)\n",
    "               for sex, color in sex_colors.items()]\n",
    "    plt.legend(handles=handles, title=\"æ€§åˆ¥\")\n",
    "    plt.xlabel(\"äººæ•°\")\n",
    "    plt.ylabel(\"å¹´ - æ€§åˆ¥\")\n",
    "    plt.title(\"å¹³å‡äººæ•°ï¼ˆå¹´ãƒ»æ€§åˆ¥åˆ¥ï¼‰\")\n",
    "    plt.grid(axis='x', linestyle=\"--\", alpha=0.7)\n",
    "    plt.savefig(fig2_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def initialize_git_repo():\n",
    "    print(\"ğŸ”§ Git ãƒªãƒã‚¸ãƒˆãƒªã‚’åˆæœŸåŒ–ä¸­...\")\n",
    "    os.chdir(GITHUB_REPO_PATH)\n",
    "\n",
    "    # .git ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãªã„å ´åˆã®ã¿åˆæœŸåŒ–\n",
    "    if not os.path.isdir(os.path.join(GITHUB_REPO_PATH, \".git\")):\n",
    "        subprocess.run([\"git\", \"init\"])\n",
    "        subprocess.run([\"git\", \"remote\", \"add\", \"origin\", GITHUB_REMOTE_URL])\n",
    "    else:\n",
    "        print(\"âœ… æ—¢ã« Git ãƒªãƒã‚¸ãƒˆãƒªã¨ã—ã¦åˆæœŸåŒ–æ¸ˆã¿\")\n",
    "\n",
    "# === GitHub ã¸ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ ===\n",
    "def push_to_github(repo_path: str):\n",
    "    os.chdir(repo_path)\n",
    "    commit_message = f\"Update graphs {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\n",
    "\n",
    "    subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", \"å–å¾—å¿…è¦\"])  # â† è‡ªåˆ†ã®åå‰ã«å¤‰æ›´\n",
    "    subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", \"å–å¾—å¿…è¦\"])  # â† è‡ªåˆ†ã®ãƒ¡ãƒ¼ãƒ«ã«å¤‰æ›´\n",
    "\n",
    "    subprocess.run([\"git\", \"add\", \".\"])\n",
    "    subprocess.run([\"git\", \"commit\", \"-m\", commit_message])\n",
    "\n",
    "    # ãƒ–ãƒ©ãƒ³ãƒåã‚’å–å¾—ï¼ˆä¾‹: main, masterï¼‰\n",
    "    result = subprocess.run([\"git\", \"branch\", \"--show-current\"], stdout=subprocess.PIPE, text=True)\n",
    "    current_branch = result.stdout.strip()\n",
    "\n",
    "    # åˆå› pushï¼ˆ--set-upstream ã‚’ä»˜ã‘ã‚‹ï¼‰\n",
    "    subprocess.run([\"git\", \"push\", \"--set-upstream\", \"origin\", current_branch])\n",
    "\n",
    "\n",
    "# === ãƒ¡ã‚¤ãƒ³å‡¦ç† ===\n",
    "def main():\n",
    "    print(\"ğŸ“¥ BigQuery ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...\")\n",
    "    df = load_data_from_bigquery()\n",
    "\n",
    "    print(\"ğŸ“Š ã‚°ãƒ©ãƒ•ä½œæˆä¸­...\")\n",
    "    create_and_save_graphs(df, LOCAL_FOLDER)\n",
    "\n",
    "    print(\"ğŸ”§ Git ãƒªãƒã‚¸ãƒˆãƒªåˆæœŸåŒ–ä¸­...\")\n",
    "    initialize_git_repo()  \n",
    "\n",
    "    print(\"ğŸš€ GitHub ã«ãƒ—ãƒƒã‚·ãƒ¥ä¸­...\")\n",
    "    push_to_github(GITHUB_REPO_PATH)\n",
    "\n",
    "    print(\"ğŸ‰ å®Œäº†ã—ã¾ã—ãŸï¼\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
